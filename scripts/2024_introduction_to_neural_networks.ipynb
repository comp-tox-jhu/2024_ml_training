{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Introduction to Neural Networks\n",
        "subtitle: ''\n",
        "editor: visual\n",
        "author: Jason Laird\n",
        "date: 06/12/2024\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-location: left\n",
        "    theme:\n",
        "      - flatly\n",
        "    linkcolor: '#555580'\n",
        "---"
      ],
      "id": "f638b175"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Packages\n",
        "\n",
        "<https://www.kaggle.com/code/ryanholbrook/a-single-neuron>\n",
        "\n",
        "<https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/?ref=lbp>\n",
        "\n",
        "<https://www.kaggle.com/learn/computer-vision>\n",
        "\n",
        "\n",
        "\n",
        "## Load Data\n",
        "\n",
        "We split the dataset into training and test sets using $X_{train}$, $$X_{test}$$, $$Y_{train}$$, and $$Y_{test}$$. We then standardize our data to ensure each feature $X_i$â€‹ has a mean of 0 and a standard deviation of 1:\n",
        "\n",
        "ð‘‹scaled=ð‘‹âˆ’ðœ‡ðœŽXscaledâ€‹=ÏƒXâˆ’Î¼â€‹ where ðœ‡Î¼ is the mean and ðœŽÏƒ is the standard deviation of the feature.\n"
      ],
      "id": "7fb9bd26"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"C:/Users/Jason/Documents/jhu/projects/2024_ml_training/results/brainspan_log.csv\")\n",
        "\n",
        "data = pd.read_csv(\"../results/brainspan_log.csv\")\n",
        "\n",
        "# Separate features (X) and labels (y)\n",
        "X = data.drop(columns=['gender'])\n",
        "y = data['gender']\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the feature columns\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "id": "b700e80e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Build a Basic Neural Network with One Node**\n",
        "\n",
        "1.  **Initialization:** We create a Sequential model which will stack layers sequentially.\n",
        "\n",
        "2.  **Single Neuron Layer:**\n",
        "\n",
        "    -   **Dense Layer:** A fully connected layer with one neuron.\n",
        "\n",
        "    -   **Input Dimension:** input_dim=ð‘›input_dim=n where ð‘›n is the number of input features.\n",
        "\n",
        "    -   **Activation Function:** Sigmoid function ðœŽ(ð‘§)=11+ð‘’âˆ’ð‘§Ïƒ(z)=1+eâˆ’z1â€‹.\n",
        "\n",
        "3.  **Loss Function:**\n",
        "\n",
        "    -   **Binary Cross-Entropy:** For binary classification, the loss function is: ð¿(ð‘¦,ð‘¦^)=âˆ’1ð‘šâˆ‘ð‘–=1ð‘š\\[ð‘¦ð‘–logâ¡(ð‘¦ð‘–)+(1âˆ’ð‘¦ð‘–)logâ¡(1âˆ’ð‘¦^ð‘–)\\]L(y,yâ€‹)=âˆ’m1â€‹âˆ‘i=1mâ€‹\\[yiâ€‹log(y^â€‹iâ€‹)+(1âˆ’yiâ€‹)log(1âˆ’y^â€‹iâ€‹)\\] where ð‘¦y is the true label and ð‘¦^y^â€‹ is the predicted probability.\n",
        "\n",
        "4.  **Optimizer:** Adam optimizer adjusts weights using gradients.\n"
      ],
      "id": "dd5dcbff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Initialize the neural network\n",
        "model = Sequential()\n",
        "\n",
        "# Add a single neuron\n",
        "model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=10, validation_data=(X_test, y_test))"
      ],
      "id": "3d517d6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Build a More Complex Neural Network for Classification**\n",
        "\n",
        "1.  **Hidden Layers:**\n",
        "\n",
        "    -   **ReLU Activation:** ReLU(ð‘§)=maxâ¡(0,ð‘§)ReLU(z)=max(0,z). Introduces non-linearity and helps with vanishing gradient problem.\n",
        "\n",
        "    -   The hidden layers consist of 64, 32, and 16 neurons respectively.\n",
        "\n",
        "2.  **Output Layer:** Same as before with a sigmoid activation for binary classification.\n",
        "\n",
        "3.  **Training:** The model learns to minimize the loss function over epochs.\n"
      ],
      "id": "3b2b586c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize the neural network\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=10, validation_data=(X_test, y_test))"
      ],
      "id": "0e02cdeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the Model\n",
        "\n",
        "1.  **Evaluation:** The model's performance is evaluated using the test set.\n",
        "\n",
        "2.  **Accuracy:** The fraction of correct predictions: Accuracy=NumberÂ ofÂ CorrectÂ PredictionsTotalÂ NumberÂ ofÂ PredictionsAccuracy=TotalÂ NumberÂ ofÂ PredictionsNumberÂ ofÂ CorrectÂ Predictionsâ€‹\n"
      ],
      "id": "bf5cc197"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy*100:.2f}%')"
      ],
      "id": "4dd12ac3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Visualize the Training Process**\n",
        "\n",
        "1.  **Visualization:** Plotting accuracy and loss over epochs helps understand the model's learning process.\n",
        "\n",
        "2.  **Training vs. Validation:** Helps detect overfitting if the training accuracy improves but validation accuracy plateaus or decreases.\n"
      ],
      "id": "60d84de3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "7a3cca9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Make Predictions**\n",
        "\n",
        "1.  **Predictions:** The model outputs probabilities.\n",
        "\n",
        "2.  **Thresholding:** Convert probabilities to class labels (0 or 1) based on a threshold of 0.5.\n"
      ],
      "id": "f9effb0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "predictions = (predictions > 0.5).astype(int)"
      ],
      "id": "b0fb7eda",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}